I originally had a bunch of dependency issues on a variety of projects related to VITS, but I got used a miniconda3 virtual environment with python=3.9, and installing requirements.txt from there just worked nicely. I did also have to install espeak-ng manually for some of the code to work, but that might have been specific to my device or irrelevant to the actual VITS model.

Training was difficult to get to work at first. I had to set mixed_precision=False to prevent a runtime error, I had to set the batch size to 1 so that it wouldn't silently fail, and I added freeze support because it was recommended after an error (not sure if this was necessary or if the only problem was mixed_precision at this point). I would recommend using the train file I have now and adjust it from there instead of using the one they give you. There's not many differences, but it might save a headache or two.

Training takes quite a long time. Right now, we're looking at somewhere around an hour and a half per 10 epochs on CPU with 276 audio files, and 10 epochs is definitely not enough to get it to converge. For reference, the default is 1000 epochs, and another project I saw had 10000 epochs. I intend to test this out with my laptops GPU, which actually supports CUDA unlike my AMD GPU on my desktop. I suspect fine tuning a pre-trained model might be the way to go, which I'm imagining should be similar to the current training process, but loading pre-existing model weights beforehand.
